{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba59162",
   "metadata": {},
   "source": [
    "# ThreeService ML — Runnable Dry Run + Lecture Notes\n",
    "\n",
    "This notebook is designed for **Google NotebookLM / lecture-style teaching**. It **runs the same commands** we used in the repo to demonstrate:\n",
    "\n",
    "1. **ML inference** in both supported formats\n",
    "   - Format A: `records` (already-featured rows)\n",
    "   - Format B: raw Server snapshot (`nodes/requests/shipments/batches`)\n",
    "2. The **transfer planner** endpoint (separate module; not KMeans/IsolationForest)\n",
    "3. The most important **edge cases** and what errors you’ll see\n",
    "\n",
    "---\n",
    "\n",
    "## Big picture (say this out loud)\n",
    "\n",
    "Our ML subsystem is *not* a classical supervised model that predicts a labeled outcome. It is an **unsupervised signal generator**:\n",
    "\n",
    "- **KMeans** assigns each region-time bucket to a **cluster** (similar behavior groups).\n",
    "- **IsolationForest** assigns an **anomaly score** and an **anomaly flag** (unusual behavior).\n",
    "\n",
    "The output is useful to *guide decisions*, but **it does not directly plan shipments**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24c824",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "This notebook expects you to run it from the repo with the `ml/` folder present.\n",
    "\n",
    "We will run the Python modules exactly as the Node gateway does:\n",
    "\n",
    "- `python -m src.infer --model-dir <runDir>`\n",
    "- `python -m src.transfer_planner ...`\n",
    "\n",
    "We’ll also show the intermediate engineered features for the raw snapshot path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05094ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "ML_DIR = REPO_ROOT / \"ml\"\n",
    "EXAMPLES_DIR = ML_DIR / \"examples\"\n",
    "RUN_DIR = ML_DIR / \"artifacts\" / \"20251101T153949Z\"\n",
    "\n",
    "def require_paths() -> None:\n",
    "    if not ML_DIR.exists():\n",
    "        raise RuntimeError(\n",
    "            f\"Expected ML dir at {ML_DIR}. Start this notebook from the repo root.\"\n",
    "        )\n",
    "    if not RUN_DIR.exists():\n",
    "        raise RuntimeError(f\"Expected trained run dir at {RUN_DIR}.\")\n",
    "\n",
    "def run_module(module: str, args: list[str], payload: object | None, cwd: Path) -> dict:\n",
    "    \"\"\"Run `python -m <module> <args>` with payload piped to stdin and parse JSON output.\"\"\"\n",
    "    if payload is None:\n",
    "        stdin_text = \"\"\n",
    "    elif isinstance(payload, str):\n",
    "        stdin_text = payload\n",
    "    else:\n",
    "        stdin_text = json.dumps(payload)\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        [sys.executable, \"-m\", module, *args],\n",
    "        input=stdin_text,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        cwd=str(cwd),\n",
    "    )\n",
    "\n",
    "    if proc.returncode != 0:\n",
    "        cmd = f\"{sys.executable} -m {module} {' '.join(args)}\"\n",
    "        message = (\n",
    "            f\"Command failed ({proc.returncode})\\n\"\n",
    "            f\"CMD: {cmd}\\n\\n\"\n",
    "            f\"STDOUT:\\n{proc.stdout}\\n\\n\"\n",
    "            f\"STDERR:\\n{proc.stderr}\\n\"\n",
    "        )\n",
    "        raise RuntimeError(message)\n",
    "\n",
    "    return json.loads(proc.stdout)\n",
    "\n",
    "require_paths()\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"ML_DIR:\", ML_DIR)\n",
    "print(\"RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb73e5",
   "metadata": {},
   "source": [
    "## 1) What is inside a trained run directory?\n",
    "\n",
    "A *run directory* is the ML artifact bundle created during training. Inference needs it.\n",
    "\n",
    "It contains:\n",
    "- `metadata.json` (most important: `feature_columns`)\n",
    "- `kmeans_model.joblib`\n",
    "- `isolation_forest_model.joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = RUN_DIR / 'metadata.json'\n",
    "meta = json.loads(metadata_path.read_text(encoding='utf-8'))\n",
    "\n",
    "print('feature_frequency:', meta.get('config', {}).get('feature_frequency'))\n",
    "feature_columns = meta.get('feature_summary', {}).get('feature_columns') or meta.get('feature_columns')\n",
    "print('feature_columns count:', len(feature_columns) if feature_columns else 0)\n",
    "print('first 10 feature columns:', (feature_columns or [])[:10])\n",
    "\n",
    "# This matters: inference will force incoming data into exactly these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c3a8d",
   "metadata": {},
   "source": [
    "## 2) Inference Format B (raw Server snapshot) — the ‘full pipeline’ path\n",
    "\n",
    "This is what happens when Backend-A sends `nodes/requests/shipments/batches` to the ML service.\n",
    "\n",
    "Pipeline steps (lecture-friendly):\n",
    "1. Convert JSON arrays into DataFrames\n",
    "2. Feature engineer into **region+time buckets** (`state`, `district`, `period_start`)\n",
    "3. Align to `feature_columns` from `metadata.json`\n",
    "4. Predict `cluster_id` and `anomaly_score`\n",
    "\n",
    "We use a small example snapshot file that intentionally includes:\n",
    "- request `items` as list AND as dict AND as null\n",
    "- shipment `batchIds` as list AND as single value\n",
    "- batch missing `freshnessPct` / `shelf_life_hours` to trigger defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_server = json.loads(\n",
    "    (EXAMPLES_DIR / \"predict_server_snapshot_example.json\").read_text(encoding=\"utf-8\")\n",
    " )\n",
    "\n",
    "result_server = run_module(\n",
    "    module=\"src.infer\",\n",
    "    args=[\"--model-dir\", str(RUN_DIR)],\n",
    "    payload=payload_server,\n",
    "    cwd=ML_DIR,\n",
    " )\n",
    "\n",
    "print(\"count:\", result_server[\"count\"])\n",
    "print(\"missing_feature_columns (count):\", len(result_server.get(\"missing_feature_columns\", [])))\n",
    "\n",
    "for row in result_server[\"results\"]:\n",
    "    print(\"\\n---\")\n",
    "    summary = {\n",
    "        k: row.get(k)\n",
    "        for k in [\"state\", \"district\", \"period_start\", \"cluster_id\", \"is_anomaly\", \"anomaly_score\"]\n",
    "    }\n",
    "    print(summary)\n",
    "    for k in [\"requested_kg\", \"incoming_kg\", \"outgoing_kg\", \"produced_kg\", \"avg_travel_time_minutes\"]:\n",
    "        if k in row:\n",
    "            print(f\"{k}: {row[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c7ff1",
   "metadata": {},
   "source": [
    "### 2.1 (Optional) Show the engineered feature rows *before* modeling\n",
    "\n",
    "This makes the model’s behavior easy to teach: it’s operating on aggregated numeric columns.\n",
    "\n",
    "We call `prepare_feature_frame(...)` directly (same code training uses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.feature_engineering import prepare_feature_frame\n",
    "\n",
    "nodes_df = pd.DataFrame(payload_server.get('nodes') or [])\n",
    "requests_df = pd.DataFrame(payload_server.get('requests') or [])\n",
    "shipments_df = pd.DataFrame(payload_server.get('shipments') or [])\n",
    "batches_df = pd.DataFrame(payload_server.get('batches') or [])\n",
    "\n",
    "features_df, feature_meta = prepare_feature_frame(\n",
    "    nodes_df=nodes_df,\n",
    "    requests_df=requests_df,\n",
    "    shipments_df=shipments_df,\n",
    "    batches_df=batches_df,\n",
    "    freq=str(payload_server.get('freq') or 'M'),\n",
    "    festival_csv_path=None,\n",
    "    income_csv_path=None,\n",
    ")\n",
    "\n",
    "print('feature rows:', len(features_df))\n",
    "print('key columns:', ['state','district','period_start'])\n",
    "print('engineered numeric columns (sample):', [c for c in features_df.columns if c not in ['state','district','period_start']][:12])\n",
    "display(features_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21edc6e4",
   "metadata": {},
   "source": [
    "## 3) Inference Format A (`records`) — the ‘already-featured’ path\n",
    "\n",
    "Here we skip feature engineering and give feature rows directly.\n",
    "\n",
    "Key teaching point: **the model forces the input into `feature_columns` from metadata**.\n",
    "So if your record includes only a handful of numeric features, inference adds the rest as `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0675576",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_records = json.loads((EXAMPLES_DIR / 'predict_records_example.json').read_text(encoding='utf-8'))\n",
    "result_records = run_module(\n",
    "    module='src.infer',\n",
    "    args=['--model-dir', str(RUN_DIR)],\n",
    "    payload=payload_records,\n",
    "    cwd=ML_DIR,\n",
    ")\n",
    "\n",
    "print('count:', result_records['count'])\n",
    "print('missing_feature_columns (count):', len(result_records.get('missing_feature_columns', [])))\n",
    "print('example missing feature columns (first 8):', result_records.get('missing_feature_columns', [])[:8])\n",
    "\n",
    "for row in result_records['results']:\n",
    "    print({k: row[k] for k in ['state','district','period_start','cluster_id','is_anomaly','anomaly_score']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c2dbb",
   "metadata": {},
   "source": [
    "## 4) Transfer planner (separate module) — runnable example\n",
    "\n",
    "This is a different Python module: `src.transfer_planner`.\n",
    "\n",
    "It does **inventory balancing**, not clustering/anomaly detection. It consumes:\n",
    "- `nodes[]` with capacities and coordinates\n",
    "- `batches[]` with current inventory positions\n",
    "\n",
    "And returns suggestions in two categories:\n",
    "- `warehouse_to_warehouse`\n",
    "- `farm_to_warehouse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_transfer = json.loads((EXAMPLES_DIR / 'transfer_planner_example.json').read_text(encoding='utf-8'))\n",
    "plan = run_module(\n",
    "    module='src.transfer_planner',\n",
    "    args=['--mode','all','--max-pairs','5','--min-transfer-kg','200','--overstock-ratio','0.8','--understock-ratio','0.4','--target-ratio','0.6'],\n",
    "    payload=payload_transfer,\n",
    "    cwd=ML_DIR,\n",
    ")\n",
    "\n",
    "print('counts:', plan['counts'])\n",
    "\n",
    "def summarize_suggestion(s):\n",
    "    return {\n",
    "        'type': s.get('type'),\n",
    "        'suggested_quantity_kg': s.get('suggested_quantity_kg'),\n",
    "        'distance_km': s.get('distance_km'),\n",
    "        'source': (s.get('source') or {}).get('mongoId'),\n",
    "        'target': (s.get('target') or {}).get('mongoId'),\n",
    "    }\n",
    "\n",
    "print('\n",
    "Warehouse→Warehouse:')\n",
    "for s in plan.get('warehouse_to_warehouse', []):\n",
    "    print(summarize_suggestion(s))\n",
    "\n",
    "print('\n",
    "Farm→Warehouse:')\n",
    "for s in plan.get('farm_to_warehouse', []):\n",
    "    print(summarize_suggestion(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba9baa",
   "metadata": {},
   "source": [
    "## 5) ‘All the cases’: Edge cases you should be able to explain\n",
    "\n",
    "In a lecture, it’s powerful to show that the system is defensive and predictable.\n",
    "\n",
    "Below we run a few **intentional failures** and explain what the error means.\n",
    "\n",
    "### Case 5.1 — Empty payload\n",
    "Expected: `ValueError(\"No inference payload supplied.\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = run_module('src.infer', ['--model-dir', str(RUN_DIR)], payload='', cwd=ML_DIR)\n",
    "except Exception as exc:\n",
    "    print(type(exc).__name__)\n",
    "    print(str(exc)[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0607f4",
   "metadata": {},
   "source": [
    "### Case 5.2 — Payload is not `records` and not a Server snapshot\n",
    "Expected: `ValueError(\"Payload must be an object with 'records' or raw Server data...\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bad_payload = {'hello': 'world'}\n",
    "    _ = run_module('src.infer', ['--model-dir', str(RUN_DIR)], payload=bad_payload, cwd=ML_DIR)\n",
    "except Exception as exc:\n",
    "    print(type(exc).__name__)\n",
    "    print(str(exc)[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadead43",
   "metadata": {},
   "source": [
    "### Case 5.3 — Server snapshot provided but feature engineering yields 0 rows\n",
    "\n",
    "This commonly happens if there are no requests/shipments/batches (and no festival/income blocks).\n",
    "Expected: `ValueError(\"No feature rows could be generated from provided Server data.\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e574f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    emptyish_snapshot = {\n",
    "        'freq': 'M',\n",
    "        'nodes': payload_server.get('nodes') or [],\n",
    "        'requests': [],\n",
    "        'shipments': [],\n",
    "        'batches': [],\n",
    "    }\n",
    "    _ = run_module('src.infer', ['--model-dir', str(RUN_DIR)], payload=emptyish_snapshot, cwd=ML_DIR)\n",
    "except Exception as exc:\n",
    "    print(type(exc).__name__)\n",
    "    print(str(exc)[:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743c5aa",
   "metadata": {},
   "source": [
    "### Case 5.4 — Training guardrails (demonstrated locally, without MongoDB)\n",
    "\n",
    "Training requires at least **2 aggregated rows** and at least **1 numeric feature column**.\n",
    "We can show these validations by calling the training helpers directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.models import build_feature_matrix, train_unsupervised_models\n",
    "from src.config import load_config\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "# (a) No numeric columns\n",
    "try:\n",
    "    no_numeric = pd.DataFrame([{'state': 'X', 'district': 'Y', 'period_start': '2026-01-01T00:00:00'}])\n",
    "    _matrix, _cols = build_feature_matrix(no_numeric)\n",
    "except Exception as exc:\n",
    "    print('No numeric columns ->', type(exc).__name__, str(exc))\n",
    "\n",
    "# (b) Only 1 sample row for unsupervised models\n",
    "try:\n",
    "    one_row = pd.DataFrame([{'f1': 1.0, 'f2': 2.0}])\n",
    "    _ = train_unsupervised_models(one_row, config)\n",
    "except Exception as exc:\n",
    "    print('Need >= 2 samples ->', type(exc).__name__, str(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc785846",
   "metadata": {},
   "source": [
    "## 6) Practical lecture summary (copy/paste)\n",
    "\n",
    "- The ML model trains on **aggregated region+time rows**, not raw shipments/requests.\n",
    "- It outputs **clusters** (KMeans) and **anomaly signals** (IsolationForest).\n",
    "- Inference supports 2 input formats:\n",
    "  - feature rows (`records`)\n",
    "  - raw snapshot (`nodes/requests/shipments/batches`)\n",
    "- Inference is robust: missing expected numeric features are filled with **0** using the run’s `feature_columns` schema.\n",
    "- Transfer planner is a **separate** module that proposes inventory balancing transfers using capacities + distances.\n",
    "\n",
    "If you want the cleanest demos, retrain the run in the same sklearn version you’ll use for inference (avoids pickle version warnings)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
